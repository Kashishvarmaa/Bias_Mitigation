{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Mitigation using Reweighting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example synthetic dataset\n",
    "# X: Features, y: Labels, s: Sensitive attribute (e.g., gender, race)\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 5)  # 1000 samples, 5 features\n",
    "y = np.random.randint(0, 2, 1000)  # Binary target labels\n",
    "s = np.random.randint(0, 2, 1000)  # Sensitive attribute (e.g., gender 0 or 1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(X, y, s, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 1: Calculate reweighting factors\n",
    "def calculate_weights(sensitive_attribute, labels):\n",
    "    # Count frequency of each combination of sensitive attribute and label\n",
    "    unique, counts = np.unique(np.stack((sensitive_attribute, labels), axis=1), axis=0, return_counts=True)\n",
    "    \n",
    "    # Calculate total number of instances per label and per sensitive attribute\n",
    "    label_counts = np.bincount(labels)\n",
    "    sensitive_counts = np.bincount(sensitive_attribute)\n",
    "    \n",
    "    # Compute weights for each group\n",
    "    weights = np.zeros_like(labels, dtype=float)\n",
    "    \n",
    "    for (s_val, y_val), count in zip(unique, counts):\n",
    "        # Weight = total instances / (instances in this group)\n",
    "        weights[(sensitive_attribute == s_val) & (labels == y_val)] = (len(labels) / count)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Step 2: Apply bias mitigation by adjusting sample weights\n",
    "weights = calculate_weights(s_train, y_train)\n",
    "\n",
    "# Step 3: Train model using sample weights\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train, sample_weight=weights)\n",
    "\n",
    "# Step 4: Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Bias mitigation: Test for fairness\n",
    "def demographic_parity_difference(predictions, sensitive_attribute):\n",
    "    # Calculate demographic parity: P(y_pred = 1 | s = 0) - P(y_pred = 1 | s = 1)\n",
    "    p_y_pred_1_s0 = np.mean(predictions[sensitive_attribute == 0] == 1)\n",
    "    p_y_pred_1_s1 = np.mean(predictions[sensitive_attribute == 1] == 1)\n",
    "    return p_y_pred_1_s0 - p_y_pred_1_s1\n",
    "\n",
    "dp_diff = demographic_parity_difference(y_pred, s_test)\n",
    "print(f\"Demographic Parity Difference: {dp_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing:\n",
    "\n",
    "In this stage, we modify the dataset to remove bias. We will use the Reweighing method from AIF360 to adjust the dataset distribution.\n",
    "\n",
    "2. In-processing:\n",
    "\n",
    "This involves using a bias-mitigating algorithm during training. We will use the Adversarial Debiasing technique here.\n",
    "\n",
    "3. Post-processing:\n",
    "\n",
    "After the model is trained, post-processing ensures fairness by adjusting the predictions. We will use the Equalized Odds method for this.\n",
    "\n",
    "All this is performed in the code below \n",
    "\n",
    "\n",
    "The code will print the accuracy of the model before and after post-processing and the Statistical Parity Difference before and after bias mitigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.sklearn.metrics import statistical_parity_difference\n",
    "from tensorflow.compat.v1 import disable_v2_behavior, Session\n",
    "\n",
    "# Disable TensorFlow 2 behavior for AIF360 compatibility\n",
    "disable_v2_behavior()\n",
    "\n",
    "# Load dataset: Adult Income dataset from AIF360\n",
    "dataset = AdultDataset()\n",
    "\n",
    "# Split dataset into train and test\n",
    "train, test = dataset.split([0.7], shuffle=True)\n",
    "\n",
    "# Preprocessing: Reweighing to ensure fairness in the dataset\n",
    "RW = Reweighing(unprivileged_groups=[{'sex': 0}], privileged_groups=[{'sex': 1}])\n",
    "train_transf = RW.fit_transform(train)\n",
    "\n",
    "# In-processing: Adversarial Debiasing model to mitigate bias during training\n",
    "sess = Session()\n",
    "model = AdversarialDebiasing(privileged_groups=[{'sex': 1}], unprivileged_groups=[{'sex': 0}], \n",
    "                             scope_name='debiasing_classifier', sess=sess)\n",
    "model.fit(train_transf)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "test_pred = model.predict(test)\n",
    "accuracy = accuracy_score(test_pred.labels, test.labels)\n",
    "print(f\"Accuracy (In-processing): {accuracy:.4f}\")\n",
    "\n",
    "# Bias Detection: Check for statistical parity difference before post-processing\n",
    "metric = BinaryLabelDatasetMetric(test, unprivileged_groups=[{'sex': 0}], privileged_groups=[{'sex': 1}])\n",
    "spd_before = metric.statistical_parity_difference()\n",
    "print(f\"Statistical Parity Difference (before post-processing): {spd_before:.4f}\")\n",
    "\n",
    "# Post-processing: Equalized Odds post-processing to further reduce bias\n",
    "eq_odds = EqOddsPostprocessing(privileged_groups=[{'sex': 1}], unprivileged_groups=[{'sex': 0}])\n",
    "eq_odds.fit(test, test_pred)\n",
    "test_pred_transf = eq_odds.predict(test_pred)\n",
    "\n",
    "# Bias Detection after post-processing\n",
    "metric_post = BinaryLabelDatasetMetric(test_pred_transf, unprivileged_groups=[{'sex': 0}], privileged_groups=[{'sex': 1}])\n",
    "spd_after = metric_post.statistical_parity_difference()\n",
    "print(f\"Statistical Parity Difference (after post-processing): {spd_after:.4f}\")\n",
    "\n",
    "# Final accuracy after post-processing\n",
    "accuracy_post = accuracy_score(test_pred_transf.labels, test.labels)\n",
    "print(f\"Accuracy (Post-processing): {accuracy_post:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pre processing \n",
    "\n",
    "\n",
    "This stage adjusts the dataset to reduce bias before training. We’ll use the Reweighing technique.\n",
    "\n",
    "Bias detection method used: Statistical Parity Difference is calculated after reweighing to check if the dataset has less bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "# Load dataset\n",
    "dataset = AdultDataset()\n",
    "\n",
    "# Split dataset into train and test\n",
    "train, test = dataset.split([0.7], shuffle=True)\n",
    "\n",
    "# Preprocessing: Reweighing to ensure fairness\n",
    "RW = Reweighing(unprivileged_groups=[{'sex': 0}], privileged_groups=[{'sex': 1}])\n",
    "train_transf = RW.fit_transform(train)\n",
    "\n",
    "# Bias Detection: Statistical Parity Difference after preprocessing\n",
    "metric = BinaryLabelDatasetMetric(train_transf, unprivileged_groups=[{'sex': 0}], privileged_groups=[{'sex': 1}])\n",
    "spd = metric.statistical_parity_difference()\n",
    "print(f\"Statistical Parity Difference after Reweighing: {spd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In processing \n",
    "\n",
    "\n",
    "This approach integrates bias mitigation into the model training process. We’ll use Adversarial Debiasing during training.\n",
    "\n",
    "Bias detection method used: The Statistical Parity Difference and Accuracy are evaluated after training the model to check for bias and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from tensorflow.compat.v1 import disable_v2_behavior, Session\n",
    "\n",
    "# Disable TensorFlow 2 behavior for AIF360 compatibility\n",
    "disable_v2_behavior()\n",
    "\n",
    "# Load dataset\n",
    "dataset = AdultDataset()\n",
    "\n",
    "# Split dataset into train and test\n",
    "train, test = dataset.split([0.7], shuffle=True)\n",
    "\n",
    "# In-processing: Adversarial Debiasing model to mitigate bias during training\n",
    "sess = Session()\n",
    "model = AdversarialDebiasing(privileged_groups=[{'sex': 1}], unprivileged_groups=[{'sex': 0}], \n",
    "                             scope_name='debiasing_classifier', sess=sess)\n",
    "model.fit(train)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "test_pred = model.predict(test)\n",
    "accuracy = accuracy_score(test_pred.labels, test.labels)\n",
    "print(f\"Accuracy after In-processing (Adversarial Debiasing): {accuracy:.4f}\")\n",
    "\n",
    "# Bias Detection: Statistical Parity Difference after in-processing\n",
    "metric = BinaryLabelDatasetMetric(test_pred, unprivileged_groups=[{'sex': 0}], privileged_groups=[{'sex': 1}])\n",
    "spd = metric.statistical_parity_difference()\n",
    "print(f\"Statistical Parity Difference after Adversarial Debiasing: {spd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Post processing \n",
    "\n",
    "\n",
    "This stage adjusts the predictions of a trained model to improve fairness after training. We’ll use the Equalized Odds method.\n",
    "\n",
    "Bias detection method used: Statistical Parity Difference and Accuracy to check if bias was reduced post-processing and how it affects the model’s performance.\n",
    "\n",
    "\n",
    "Equalized Odds is applied after the model makes predictions, adjusting the outputs to reduce bias and ensure fairness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "# Load dataset\n",
    "dataset = AdultDataset()\n",
    "\n",
    "# Split dataset into train and test\n",
    "train, test = dataset.split([0.7], shuffle=True)\n",
    "\n",
    "# For simplicity, let's assume we have already trained a model\n",
    "# and have predictions on the test set (here we just copy the labels for demonstration).\n",
    "test_pred = test.copy()\n",
    "\n",
    "# Post-processing: Equalized Odds post-processing to reduce bias\n",
    "eq_odds = EqOddsPostprocessing(privileged_groups=[{'sex': 1}], unprivileged_groups=[{'sex': 0}])\n",
    "eq_odds.fit(test, test_pred)\n",
    "test_pred_transf = eq_odds.predict(test_pred)\n",
    "\n",
    "# Bias Detection: Statistical Parity Difference after post-processing\n",
    "metric = BinaryLabelDatasetMetric(test_pred_transf, unprivileged_groups=[{'sex': 0}], privileged_groups=[{'sex': 1}])\n",
    "spd = metric.statistical_parity_difference()\n",
    "print(f\"Statistical Parity Difference after Equalized Odds Post-processing: {spd:.4f}\")\n",
    "\n",
    "# Evaluate the final accuracy after post-processing\n",
    "accuracy_post = accuracy_score(test_pred_transf.labels, test.labels)\n",
    "print(f\"Accuracy after Post-processing (Equalized Odds): {accuracy_post:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
