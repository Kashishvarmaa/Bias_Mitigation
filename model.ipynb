{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.metrics import ClassificationMetric\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/mnt/data/Student Feedback form_IDA1_CS1202 - Student Feedback form_IDA1_CS1202.csv.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Basic description of the dataset\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values if any\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Convert categorical 'Gender' column to numerical representation (0 for Male, 1 for Female)\n",
    "df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Target and Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['Feedback Rating'])  # Drop target column\n",
    "y = df['Feedback Rating']                 # Define the target\n",
    "\n",
    "# Sensitive feature (Gender)\n",
    "sensitive_feature = 'Gender'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Dataset into AIF360 Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Convert the dataset into AIF360 BinaryLabelDataset format\n",
    "data = BinaryLabelDataset(df=pd.concat([X, y], axis=1),\n",
    "                          label_names=['Feedback Rating'],\n",
    "                          protected_attribute_names=['Gender'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = data.split([0.8], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing: Reweighing the Dataset to Mitigate Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweigher = Reweighing(unprivileged_groups=[{'Gender': 0}], privileged_groups=[{'Gender': 1}])\n",
    "train_reweighed = reweigher.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Processing: Adversarial Debiasing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up and Train the Adversarial Debiasing Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Adversarial Debiasing model\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create the AdversarialDebiasing model\n",
    "adv_debiasing = AdversarialDebiasing(privileged_groups=[{'Gender': 1}],   # Privileged group: Female\n",
    "                                     unprivileged_groups=[{'Gender': 0}],  # Unprivileged group: Male\n",
    "                                     scope_name='debiasing_classifier',\n",
    "                                     debias=True,                         # Enable debiasing\n",
    "                                     sess=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "adv_debiasing.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions with the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "predictions = adv_debiasing.predict(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model's Fairness and Performance after In Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Evaluation - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "test_labels = test.labels\n",
    "pred_labels = predictions.labels\n",
    "\n",
    "accuracy = accuracy_score(test_labels, pred_labels)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy_pre = accuracy_score(test.labels, predictions.labels)\n",
    "print(f\"Test Accuracy (Pre Post-processing): {accuracy_pre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Compute fairness metrics\n",
    "metric = ClassificationMetric(test, predictions,\n",
    "                              unprivileged_groups=[{'Gender': 0}],\n",
    "                              privileged_groups=[{'Gender': 1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness: Disparate Impact and Equal Opportunity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disparate_impact_pre = metric_pre.disparate_impact()\n",
    "equal_opportunity_pre = metric_pre.equal_opportunity_difference()\n",
    "\n",
    "print(f\"Disparate Impact (Pre Post-processing): {disparate_impact_pre}\")\n",
    "print(f\"Equal Opportunity Difference (Pre Post-processing): {equal_opportunity_pre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic parity\n",
    "print(f\"Demographic Parity Difference: {metric.disparate_impact()}\")\n",
    "\n",
    "# Equal opportunity difference\n",
    "print(f\"Equal Opportunity Difference: {metric.equal_opportunity_difference()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing - Equalized Odds Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_odds = EqOddsPostprocessing(privileged_groups=[{'Gender': 1}], \n",
    "                               unprivileged_groups=[{'Gender': 0}])\n",
    "\n",
    "predictions_post = eq_odds.fit_predict(test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance and Fairness After Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy_post = accuracy_score(test.labels, predictions_post.labels)\n",
    "print(f\"Test Accuracy (Post Post-processing): {accuracy_post:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric_post = ClassificationMetric(test, predictions_post,\n",
    "                                   unprivileged_groups=[{'Gender': 0}],\n",
    "                                   privileged_groups=[{'Gender': 1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness: Disparate Impact and Equal Opportunity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disparate_impact_post = metric_post.disparate_impact()\n",
    "equal_opportunity_post = metric_post.equal_opportunity_difference()\n",
    "\n",
    "print(f\"Disparate Impact (Post Post-processing): {disparate_impact_post}\")\n",
    "print(f\"Equal Opportunity Difference (Post Post-processing): {equal_opportunity_post}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Bias Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Demographic Parity (Disparate Impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot demographic parity difference\n",
    "plt.bar(['Demographic Parity Difference'], [metric.disparate_impact()])\n",
    "plt.title('Demographic Parity Difference')\n",
    "plt.show()\n",
    "\n",
    "# Plot equal opportunity difference\n",
    "plt.bar(['Equal Opportunity Difference'], [metric.equal_opportunity_difference()])\n",
    "plt.title('Equal Opportunity Difference')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Equal Opportunity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['Pre Post-processing', 'Post Post-processing'], [equal_opportunity_pre, equal_opportunity_post])\n",
    "plt.title('Equal Opportunity Difference')\n",
    "plt.ylabel('Equal Opportunity Difference')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
